## 一、什么是爬虫? 
    网络爬虫，是一种按照一定规则，自动取抓取互联网上的信息的程序。也就是模拟客户端发起网络请求，
    接受请求的响应，按照一定的规则，自动的抓取互联网信息的程序。
    理论上：通过浏览器看到的数据,我们一般都是可以获取到的


## 二、爬虫的基本流程?
    1.分析网站，得到目标url
    2.根据url,发起请求,获取页面的HTML源码
    3.从页面源码中提取数据
        (1).提取到目标数据,做数据的赛选和持久化存储
        (2).从页面中提取到新的url地址,继续执行第二部操作
    4.爬虫结束：所有的目标url都提取完毕,并且得到数据了,再也没有其他请求任务了,这是意味着爬虫结束


## 三、网页的三大特征?
    1.每一个网页都有一个唯一的url(统一资源定位符),来进行定位
    2.网页都是通过HTML(超文本)文本展示的
    3.所有的网页都是通过HTTP＜超文本传输协议＞(HTTPS)协议来传输的


## 四、通用爬虫和聚焦爬虫?
    1),通用爬虫： 是搜索引擎的重要组成部分
        目的：尽可能的将所有的互联网上的网页下载到本地,经过预处理（去噪,分词,去广告...）,最终将数据存储到本地,做一个镜像备份,形成一个检索系统.
    2),聚焦爬虫：聚焦爬虫是面向主题的爬虫,再爬取数据的过程中会对数据进行筛选,往往只会爬虫与需求相关的数据


## 五、robot协议：
    就是一个规范,告诉搜索引擎,哪些目录下的资源允许爬虫,哪些目录下的资源不允许爬取.
    其中：'User-agent':该项值用来表示是哪家的搜索引擎
        'allow':允许被爬取的url
     　 'disllow':不允许被爬取的url


## 六、七层协议(从上往下)：
	应用层： 为用户的应用程序提供网络服务的（http,https,ftp....）
	表示层： 负责端到端的数据信息可以被另一个主机所理解和识别,并且按照一定的格式将信息传递到会话层
	会话层： 会话层是管理主机之间的会话进程，负责建立，管理，和终止会话进程
	传输层： 进行数据传输的（TCP UDP)
	网络层： 路由器
	数据链路层：　网桥、交换机
	物理层：　网线、网卡、集线器、中继器
|| tcp| udp |
| ------| :------: | :------: |
| 传输方式上　 | 面向连接的 | 非面向连接的 |
| 传输的数据 |传输的是数据流 | 传输的是数据包（可能会出现丢包） |
| 传输的效率 | 慢 | 快 |
| 传输的稳定性 | 高 | 低 |

---

## 七、请求常见的状态码:
    200:请求成功
    301:永久重定向
    302:临时重定向
    400:客户端错误
    401:未授权
    403:服务器拒绝访问
    404:页面未找到
    405:请求方式不对
    408:请求超时
    500:服务器错误
    503:服务器不可用


## 八、Http和Https协议:
    HTTP协议（HyperText Transfer Protocol）中文名叫超文本传输协议： 是用于从网络传送超文本数据到本地浏览器的传送协议
    HTTPS协议（Hypertext Transfer Protocol over Secure Socket Layer）简单讲是HTTP的安全版，在HTTP协议的基础上加入SSL层(HTTP+SSL)。 
           SSL(Secure Sockets Layer 安全套接层) 。主要用于Web的安全传输协议，在传输层对网络连接进行加密，保障在Internet上数据传输的安全。
    HTTP的端口号为80， HTTPS的端口号为443
    HTTPS的安全基础是SSL,因此通过它可以传输的内容都是经过SSL加密的，主要作用是：
            1):建立一个安全有效的信息传送通道，保证数据传送的安全性
            2):确定网站的真实性和有效性
    

## 九、url的基本组成部分:
    基本格式：scheme://host[:port#]/path/…/[?query-string][#anchor]
         -> scheme：协议(例如：http, https, ftp)
         -> host：服务器的IP地址或者域名
         -> port#：服务器的端口（如果是走协议默认端口，缺省端口80）
         -> path：访问资源的路径
         -> query-string：参数，发送给http服务器的数据
         -> anchor：锚（跳转到网页的指定锚点位置


## 十、发起一个请求的基本流程:
    1)、当用户在浏览器的地址栏中输入一个URL并按回车键之后(会先经过DNS服务将域名解析为ip)，浏览器会向HTTP服务器发送HTTP请求。
    2)、当我们在浏览器输入URL http://www.baidu.com 的时候，浏览器发送一个Request请求去获取 http://www.baidu.com 的html文件，服务器把Response文件对象发送回给浏览器。
    3)、浏览器分析Response中的HTML，发现其中引用了很多其他文件，比如Images文件，CSS文件，JS文件。 浏览器会自动再次发送Request去获取图片，CSS文件，或者JS文件。
    4)、当所有的文件都下载成功后，网页会根据HTML语法结构，完整的显示出来了。


## 十一、Get请求和Post请求的区别:
    1)、GET是从服务器上获取数据，POST是向服务器传送数据
    2)、GET请求参数显示，都显示在浏览器网址上，HTTP服务器根据该请求所包含URL中的参数来产生响应内容，即“Get”请求的参数是URL的一部分。 例如： http://www.baidu.com/s?wd=Chinese
    3)、POST请求参数在请求体当中，消息长度没有限制而且以隐式的方式进行发送，通常用来向HTTP服务器提交量比较大的数据（比如请求中包含许多参数或者文件上传操作等），请求的参数包含在“Content-Type”消息头里，指明该消息体的媒体类型和编码.
    4)、避免使用Get方式提交表单，因为有可能会导致安全问题。 比如说在登陆表单中用Get方式，用户输入的用户名和密码将在地址栏中暴露无遗


## 十二、请求头参数:
    1. User-Agent (浏览器名称)
	    User-Agent：是客户浏览器的名称，以后会详细讲。
    2. Cookie （Cookie）
	    Cookie：浏览器用这个属性向服务器发送Cookie。Cookie是在浏览器中寄存的小型数据体，它可以记载和服务器相关的用户信息，也可以用来实现会话功能。
    3. Referer (页面跳转处)
	    Referer：表明产生请求的网页来自于哪个URL，用户是从该 Referer页面访问到当前请求的页面。这个属性可以用来跟踪Web请求来自哪个页面， 是从什么网站来的等。
        有时候遇到下载某网站图片，需要对应的referer，否则无法下载图片，那是因为人家做了防盗链，原理就是根据referer去判断是否是本网站的地址 ，如果不是，则拒绝，如果是，就可以下载；
    4. Content-Type (POST数据类型)
	    Content-Type：POST请求里用来表示的内容类型。
    5. Host (主机和端口号)
	    Host：对应网址URL中的Web名称和端口号，用于指定被请求资源的Internet主机和端口号，通常属于URL的一部分。
    6. X-Requested-With: XMLHttpRequest(表示是一个Ajax异步请求)
    7. Connection (链接类型) Connection：表示客户端与服务连接类型
	    Client 发起一个包含 Connection:keep-alive 的请求，HTTP/1.1使用 keep-alive 为默认值。
    8. Upgrade-Insecure-Requests (升级为HTTPS请求)
	    Upgrade-Insecure-Requests：升级不安全的请求，意思是会在加载 http 资源时自动替换成 https 请求，让浏览器不再显示https页面中的http请求警报。
	    HTTPS 是以安全为目标的 HTTP 通道，所以在 HTTPS 承载的页面上不允许出现 HTTP 请求，一旦出现就是提示或报错。
    9. Accept (传输文件类型)
	    Accept：指浏览器或其他客户端可以接受的MIME（Multipurpose Internet Mail Extensions（多用途互联网邮件扩展））文件类型，服务器可以根 据它判断并返回适当的文件格式。
    10. Accept-Encoding（文件编解码格式）
	    Accept-Encoding：指出浏览器可以接受的编码方式。编码方式不同于文件格式，它是为了压缩文件并加速文件传递速度。浏览器在接收到Web响应 之后先解码，然后再检查文件格式，许多情形下这可以减少大量的下载时间。
    11. Accept-Language（语言种类）
	    Accept-Langeuage：指出浏览器可以接受的语言种类，如en或en-us指英语，zh或者zh-cn指中文，当服务器能够提供一种以上的语言版本时要用到。
    12. Accept-Charset（字符编码） Accept-Charset：指出浏览器可以接受的字符编码。 举例：Accept-Charset:gb2312,utf-8
	    gb2312：标准简体中文字符集;
	    utf-8：UNICODE 的一种变长字符编码，可以解决多种语言文本显示问题，从而实现应用国际化和本地化。
	    如果在请求消息中没有设置这个域，缺省表示任何字符集都可以接受。
---
## 十三、响应头参数:
    1. Cache-Control：must-revalidate, no-cache, private。(是否需要缓存资源)
	    这个值告诉客户端，服务端不希望客户端缓存资源，在下次请求资源时，必须要从新请求服务器，不能从缓存副本中获取资源。
	    Cache-Control是响应头中很重要的信息，当客户端请求头中包含Cache-Control:max-age=0请求， 明确表示不会缓存服务器资源时,Cache-Control作为作为回应信息，通常会返回no-cache， 意思就是说，"那就不缓存呗"。
    2. Connection：keep-alive（保持连接） 这个字段作为回应客户端的Connection：keep-alive，告诉客户端服务器的tcp连接也是一个长连接，客户端可以继续使用这个tcp连接发送http请求。
    3. Content-Encoding:gzip（web服务器支持的返回内容压缩编码类型） 告诉客户端，服务端发送的资源是采用gzip编码的，客户端看到这个信息后，应该采用gzip对资源进行解码。
    4. Content-Type：text/html;charset=UTF-8（文件类型和字符编码格式）
	    告诉客户端，资源文件的类型，还有字符编码，客户端通过utf-8对资源进行解码，然后对资源进行html解析。
	    通常我们会看到有些网站是乱码的，往往就是服务器端没有返回正确的编码。
    5. Date：Sun, 21 Sep 2016 06:18:21 GMT（服务器消息发出的时间）
	    这个是服务端发送资源时的服务器时间，GMT是格林尼治所在地的标准时间。http协议中发送的时间都是 GMT的，这主要是解决在互联网上，不同时区在相互请求资源的时候，时间混乱问题。
    6. Expires:Sun, 1 Jan 2000 01:00:00 GMT（响应过期的日期和时间）
	    这个响应头也是跟缓存有关的，告诉客户端在这个时间前，可以直接访问缓存副本，很显然这个值会存在 问题，因为客户端和服务器的时间不一定会都是相同的，如果时间不同就会导致问题。所以这个响应头是 没有Cache-Control：max-age=*这个响应头准确的，因为max-age=date中的date是个相对时间，不仅更 好理解，也更准确。
    7. Pragma:no-cache 这个含义与Cache-Control（是否缓存资源）等同。
    8.Server：Tengine/1.4.6（服务器和服务器版本）
	    这个是服务器和相对应的版本，只是告诉客户端服务器的信息。
    9. Transfer-Encoding：chunked
	    这个响应头告诉客户端，服务器发送的资源的方式是分块发送的。一般分块发送的资源都是服务器动态生成的， 在发送时还不知道发送资源的大小，所以采用分块发送，每一块都是独立的，独立的块都能标示自己的长度， 最后一块是0长度的，当客户端读到这个0长度的块时，就可以确定资源已经传输完了。
    10. Vary: Accept-Encoding
	    告诉缓存服务器，缓存压缩文件和非压缩文件两个版本，现在这个字段用处并不大，因为现在的浏览器都是 支持压缩的。 响应状态码 响应状态代码有三位数字组成，第一个数字定义了响应的类别，且有五种可能取值。

---
## 十四、str和bytes区别:
**str类型使用encode方法转化为bytes类型 bytes类型通过decode转化为str类型**
```python
 In [1]: str1='人生苦短，我用Python!'

 In [2]: type(str1)
 Out[2]: str

 In [3]: b=str1.encode()

 In [4]: b
 Out[4]: b'\xe4\xba\xba\xe7\x94\x9f\xe8\x8b\xa6\xe7\x9f\xad\xef\xbc\x8c\xe6\x88\x91\xe7\x94\xa8Python!'

 In [5]: type(str1.encode())
 Out[5]: bytes
```
**bytes转换成str：**
```python
 In [1]: b
 Out[1]: b'\xe4\xba\xba\xe7\x94\x9f\xe8\x8b\xa6\xe7\x9f\xad\xef\xbc\x8c\xe6\x88\x91\xe7\x94\xa8Python!'

 In [2]: type(b)
 Out[2]: bytes

 In [3]: b.decode()
 Out[3]: '人生苦短，我用Python!'

 In [4]: type(b.decode())
 Out[4]: str
```
---
## 十五、发起一个Get请求
**在其中我们可以看到在请求部分里,http://www.baidu.com/s? 之后出现一个长长的字符串，其实就是我们要查询的关键词美女，于是我们可以尝试用默认的Get方式来发送请求。**
```python
# urllib_get.py

url = "http://www.baidu.com/s"
word = {"wd":"美女"}
word = urllib.parse.urlencode(word) #转换成url编码格式（字符串）
newurl = url + "?" + word # url首个分隔符就是 ?

headers={
"User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36"
}

request = urllib.request.Request(newurl, headers=headers)

response = urllib.request.urlopen(request)

html = response.read()

print (html)

```